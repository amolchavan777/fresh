

ğŸ”¹ Slide 1: The Challenge â€” Dependency Mapping in the Real World

â±ï¸ ~1 minute

Good [morning/afternoon] everyone.
In a typical enterprise environment, understanding application dependencies should be easy â€” until it isnâ€™t.

Logs say one thing. Telemetry says another. Some APIs report dependencies, some donâ€™t. CI/CD knows the topology in theory, but production behavior tells a different story.

Conflict is everywhere. And most tools just assume their data is correct.

But in practice â€” real systems are noisy, partial, contradictory.
What we need is a way to merge multiple imperfect signals into a trustworthy dependency graph.

So we asked: Can we build a system that thinks like an engineer? One that listens to all the sources, detects contradictions, and reasons out the truth?

â¸»

ğŸ”¹ Slide 2: The Engine â€” Truth Discovery Over Conflicting Claims

â±ï¸ ~1 minute 15 seconds

At the core of our solution is a truth discovery engine, based on the Expectation-Maximization algorithm â€” a statistical approach used in real-world information fusion.

Hereâ€™s how it works:
We feed it dependency claims from diverse sources â€” logs, telemetry, YAML specs, GitLab pipelines, even API contracts.
Each claim can be positive (â€œA connects to Bâ€) or negative (â€œA failed to connect to Bâ€).

The engine:
	â€¢	Assigns trust scores to each data source
	â€¢	Separates conflicting from non-conflicting claims
	â€¢	Learns which claims are likely true â€” and which are noise

It runs iteratively, updating source credibility and claim truth probabilities until it converges on a reliable model.

All of this is implemented in Java, built around adapters for different data sources â€” and yes, it supports negative evidence, which most tools ignore.

â¸»

ğŸ”¹ Slide 3: Why This Works When Others Fail

â±ï¸ ~1 minute

Most systems fail silently when data sources disagree â€” ours doesnâ€™t. Thatâ€™s the key differentiator.

Let me give you four reasons this works:
	1.	Contradiction isnâ€™t thrown out â€” itâ€™s modeled. We give every source a voice, but weigh their reliability based on past accuracy.
	2.	Data normalization is automatic. Claims from YAML specs, logs, OpenTelemetry, and GitLab are grouped, de-duped, and aligned.
	3.	Negative signals matter. A failed connection or a missing trace is used as evidence against a dependency â€” not ignored.
	4.	Final outputs are clean. We donâ€™t export raw logs; we export resolved truths â€” perfect for ArchiMate, CMDBs, and EA tooling.

So instead of drawing from guesswork or tribal knowledge, we generate a mathematically grounded dependency graph.

â¸»

ğŸ”¹ (Optional) Slide 4: Business Impact

â±ï¸ ~30 seconds

Technically, this is elegant â€” but itâ€™s also impactful.

With reliable application dependencies:
	â€¢	Root cause analysis gets faster â€” no false dependencies
	â€¢	Zero trust enforcement is easier â€” know exactly what should talk to what
	â€¢	Cloud migrations donâ€™t break â€” because we know whatâ€™s real
	â€¢	And developer onboarding is smooth â€” no more diagrams taped to walls

In a world where observability is abundant but inconsistent, we donâ€™t just observe â€” we infer the truth.

â¸»

âœ… Final Line (Call to Action)

Weâ€™ve built a system that reasons like a human, scales like software, and integrates like architecture.
We believe this isnâ€™t just the best technical solution â€” itâ€™s the most comprehensive, trustworthy, and ready-to-ship approach to building reliable application dependency maps.
